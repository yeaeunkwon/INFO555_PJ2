# Misinformation dectection with question-answering and RAG

This project explores misinformation detection using question-answering and the Retrieval-Augmented Generation (RAG) technique, structured in three main phases. In the Decomposition Phase, claims are broken down into two specific questions through a prompting strategy. Next, in the Retrieval Phase, relevant documents are retrieved from the external DuckDuckGo API and are used as evidence of each claim for baseline models. For experimental models, answers are generated from these documents either through prompting or by extracting the top five most similar sentences to each claim and question. Finally, in the Fact-Verification Phase, the claim is verified based on the generated answers and the relevant documents.

![pj2_system](https://github.com/user-attachments/assets/78a2bca8-0f78-4f27-9470-fc4a2414b698)


## Dataset
[FEVER Dataset](https://huggingface.co/datasets/fever/fever) is generated by altering sentences extracted from Wikipedia. In this project, a subset of the dataset is used, comprising 474 instances labeled as "supports" and 465 labeled as "refutes."

## Model
* Baseline: the model provided only with the original claim and the relevant documents as evidence.
* LLM-QA: the model provided with the original claim and sub-question-answer pairs as evidence where the answers are generated by LLMs provided with the top k documents and questions.
* LLM-Topk: the model provided with the original claim and sub-question-answer pairs as evidence where the top 5 most similar sentences from relevant documents are used as answers to each question.

## Code
* decomposing_question.py: the code for decomposing a claim into two questions by prompting.
* answer_generation_{model}.py: the code for generating answers to the questions by prompting.
* duckduckgo.py: the code for searching the relevant documents on the search engine API.
* Find_top_sent.py: the code for searching the top 5 most similar sentences from a relevant document.
* fact_verificaion_{model}.py: the code for the final prediction from a model.
* evaluation.py: the code for evaluation metrics
* inference_statistics: the code for inference statistics by Bonferroni-correction
* error_analysis: the code for sampling the instances labelled as "not enough info"

  
